矩阵的L0范数：矩阵的非0元素的个数，通常用来表示稀疏，L0范数越小，0元素越多，也就越稀疏
矩阵的L1范数：矩阵中每个元素绝对值之和，他是L0范数的最优凸近似，因此也可以表示稀疏

数学概念中：
凸函数：凸为正，斜率增加
凹函数：凹为负，斜率减少
如果某点转变了图像的凹凸性，这个点就是拐点
任何线性函数既是凸函数也是凹函数

最优化问题（机器学习主要工作），一般形式：
minimize f0(x)，fi(x)<=bi（bi为限制条件）,i=1,...,m
即：找到合适的性x，使得f0(x)最小

最小二乘：无约束的优化问题，通常理解为测量值与真实值之间的误差平方和：
minimize f0(x)=|||Ax-b||(2,2)=sum（a(i,T)x-bi）2
a(i,T)为A的第i行，向量x为优化目标
这个问题没有约束条件，该式子一定大于等于0，这样的优化问题，将其转化成线性方程求解
对目标和所有约束函数有定义的点的集合，称为优化问题的定义域
正则化最小二乘

线性规划：优化问题，它的目标函数和约束条件都是线性的
用画图方法，根据条件画出可行域，然后将目标函数在可行域上移动，直到得到最大值
最优化的问题中，大部分都是在寻找各种各样的方法确定步长和方向，使得迭代的速度尽可能的快，得到的解尽可能是最优的解

优化理论在机器学习、深度学习中扮演的角色：
凸优化更广泛的说是最优化理论，在目前的机器学习、数据挖掘或者是深度学习的神经网络中都要用到。
最优化就是告诉模型应该学什么，怎么学。模型学习的往往都是一个映射函数，也就是模型的参数W，这个参数的好坏得看结果才能知道，
知道错了以后，该往哪里学，怎么调整，这就是优化理论在其中扮演的角色。如果没有最优化理论，那么模型是不知道怎么学习的，也就是没有了最优化，
模型的学习永远都是停滞不前的。
很多非凸优化的问题，可以转化为凸优化问题来解决

对角矩阵：只在主对角线上含有非0元素，其他位置都是0
对称矩阵：矩阵的转置和自己相等的矩阵


******************************

集成学习：badding、boosting、stacking
集成学习的思路：
通过合并多个模型来提升机器学习的性能，相较于多个模型通常能够活得更好的预测结果，这也是集成学习被首先推荐使用的原因。

三大类：
·bagging：用于减少方差
·boosting：用于减少偏差
·stacking：用于提升预测结果

两大类：
·串行集成方法：串行地生成基础模型，如AdaBoost。基本动机是利用基础模型之间的依赖，通过给错分样本一个较大的权重来提升性能。
·并行集成方法：并行地生成基础模型，如Random Forest。基本动机是利用基础模型的独立性，因为通过平均能够较大地降低误差。

同质集成：大部分集成模型都通过一个基础学习算法来生成一个同质的基础学习器。
异质集成：为了集成后的表现最好，异质基础学习器需要尽可能准确并且差异性够大。

***Bagging（装袋）
Bagging是引导聚合的意思。减少一个估计方差的一种方式就是对多个估计进行平均。
例如用训练集的不同子集训练M个不同的树然后计算最后的结果。（求平均）
Bagging使用装袋采样来获取数据子集训练基础学习器。例如随机选择80%的数据作为训练集，同样选择80%的特征进行训练。
通常，分类任务使用投票方式集成，而回归任务提供平均的方式集成。

决策树Bagging相比K-NNBagging集成获得更高的准确率。
K-NN对于样本的扰动并不敏感，这也是为什么K-KNN称为稳定学习器的原因。
整合稳定学习器对于提升泛化性能没有帮助。

最常用的集成算法原模型是随机森林。
在随机森林中，每个数模型都是装袋采样训练的。另外特征也是随机选择的，最后对于训练好的树也是随机选择的。

这种处理的结果是随机森林的偏差增加的很少，而由于弱相关树模型的平均，方差得以降低，最终得到一个方差小，偏差也小的模型。
在一个极端的随机树算法中，随机应用的更为彻底：训练集分割的阈值也是随机的，即每次划分得到的训练集是不一样的，
这样通常能够进一步减小方差，但是会带来偏差的轻微增加。

***Boosting（提高）
Boosting是指通过算法集合将弱学习器转换为强学习器。弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是利用加权的数据。
在训练的早期对于错分数据给予较大的权重。
***boosting和bagging的区别在于：是对加权后的数据利用弱分类器依次进行训练。

对于训练好的任务，如果是分类任务，按照权重进行投票；而对于回归任务，进行加权，然后进行预测。

自适应boosting（AdaBoost），最常用的一种boosting算法。

计算单个分类器的加权错误率，正确的分类器的权重较大。

梯度树提升（Gradient Tree Boosting）是一个boosting算法在损失函数上的泛化，能够用于分类和回归问题。采用串行方式构建模型。
每新增一个决策树都尽可能地选择当前模型损失最小的那个。
注意分类和回归的算是函数有所差别。

***Stacking（堆叠）
Staking是通过一个原元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。
基础模型利用整个训练集做训练，元模型将基础模型的输出结果作为特征进行训练。

基础学习通常包含不同的算法，因此stacking通常是异质集成。


***Xgboost
本质是GBDT（Gradient Boosting Decision Tree）
残差：预测值与真实值之间的误差（真实值-当前模型预测值）
GBDT用副梯度近似残差
Xgboost与GBDT比较大的不同是目标函数的定义

Xgboost目标函数：
1.不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一棵树，其实是学习一个新的函数，去拟合上次预测的残差
2.当训练完成k棵树，预测一个样本的分布，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点对应一个分数
3.最后只需要将每棵树对应的分数加起来就是该样本的预测值。
目标函数要使得树群的预测值y'i尽量接近真实值yi，而且有尽量大的泛化能力。
从数学角度讲，这是一个泛函最优化的问题——目标函数分为两部分：误差函数和正则化项。
误差/损失函数解释训练误差（即预测分数和真实分数的差距）——预测误差尽量小
正则化定义复杂度，值越小复杂度越低，泛化能力越强——叶子节点数T尽量少，节点数值w尽量不极端（控制叶子节点的分数不会过大），防止过拟合。


***一般的目标函数都会包含两项：
误差函数：模型有多你和数据
正则化项：惩罚复杂模型
误差函数鼓励模型尽量去你和训练数据，使得最后的模型会有比较小的bias（偏差）
而正则化项鼓励更加简单的模型，因为当模型简单后，有限的数据拟合出来的结果的随机性比较小，不容易过拟合，使得模型的预测值更加稳定。

分裂节点：
一棵树的生成由一个节点一分为二，然后不断分裂最终形成整棵树。
xgboost叶子节点如何让进行分裂（两种方法）
1.枚举所有不同树结构的贪心法
不看从头到尾，就看当前节点怎么分割最好（“目光短浅”策略）
从树深度0开始，每一个节点便利所有特征，比如性别、年龄等，然后对于某个特征，先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点。
最后对所有特征进行分割后，选择增益Gain最高的那个特征（Gain：每个叶子节点对当前模型损失的贡献程度）
得到Gain值最大，意味着在一分为二这个第一曾层面上这种划分方法是最合适的。
依次华人第二层、第三层...的分裂
值得注意的是，引入分割不一定会使得情况变化，因此映入一个新子节点的惩罚项，优化这个目标对应树的剪枝，
当引入的分割带来的增益小于一个阀值的时候，则忽略这个分割。
即：当引入某项分割，结果（分割之后得到的分数-不分割得到的分数）得到的值太小（小于期望的阀值），但因此得到的复杂度过高，则相当于得不偿失，不如不分割。
2.近似算法
主要针对数据太大，不能直接进行计算


***LightGBM （Light Gradient Boosting Machine）
原文链接：https://blog.csdn.net/weixin_39807102/article/details/81912566
是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点：
更快的训练速度
更低的内存消耗
更好的准确率
分布式支持，可以快速处理海量数据

常用的机器学习算法，例如神经网络等算法，都可以以 mini-batch 的方式训练，训练数据的大小不会受到内存限制。
而 GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；
如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的

目前已有的 GBDT 工具基本都是基于预排序的方法（pre-sorted）的决策树算法(如 xgboost)。这种构建决策树的算法基本思想是： 　　
·首先，对所有特征都按照特征的数值进行预排序。 　　
·其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。 　　
·最后，找到一个特征的分割点后，将数据分裂成左右子节点。 　　
这样的预排序算法的优点是：能精确地找到分割点。 　　
缺点也很明显： 　　
·首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 　　
·其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
·最后，对 cache 优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化。
 同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的 cache miss。



LightGBM 优化部分包含以下：

基于 Histogram 的决策树算法
带深度限制的 Leaf-wise 的叶子生长策略
直方图做差加速
直接支持类别特征(Categorical Feature)
Cache 命中率优化
基于直方图的稀疏特征优化
多线程优化。

当然，histogram（直方图）算法也有缺点，它不能找到很精确的分割点，训练误差没有pre-sorted（预排序）好。
但从实验结果来看，histogram算法在测试集的误差和pre-sorted算法差异并不是很大，甚至有时候效果更好。
实际上可能决策树对于分割点的精确程度并不太敏感，而且较“粗”的分割点也自带正则化的效果。
























